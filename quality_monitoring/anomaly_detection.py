"""
Detect anomalies in key SaaS metrics.

This module identifies when metrics deviate significantly from expected patterns:
- MRR drops exceeding a configured threshold (week-over-week)
- Churn spikes above a configured multiple of rolling average

All checks query DuckDB marts tables generated by dbt.
"""

from __future__ import annotations

import logging
import os
from dataclasses import dataclass
from datetime import datetime
from typing import Optional

import duckdb
import pandas as pd

logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class AnomalyResult:
    check_name: str
    run_at_utc: datetime
    anomalies: pd.DataFrame


def _connect(db_path: str) -> duckdb.DuckDBPyConnection:
    return duckdb.connect(db_path, read_only=True)


def detect_mrr_anomalies(
    db_path: str,
    lookback_days: int = 180,
    z_threshold: float = 2.5,
    rolling_window_days: int = 7,
) -> AnomalyResult:
    """
    Detect daily MRR anomalies using z-score over a rolling window.

    Notes:
    - We compute daily MRR by spreading customer-month MRR across days in that month (simple proxy).
    - This is sufficient for portfolio monitoring; if you have true daily revenue events, swap this query.
    """
    run_at = datetime.utcnow()
    conn = _connect(db_path)

    query = f"""
        with date_spine as (
            select date_day
            from main_marts.dim_dates
            where date_day >= current_date - interval {lookback_days} day
              and date_day < current_date
        ),
        monthly_mrr as (
            select
                date_month,
                sum(mrr_amount) as total_mrr
            from main_marts.fct_monthly_recurring_revenue
            group by 1
        ),
        daily_mrr as (
            select
                d.date_day as day,
                coalesce(m.total_mrr, 0) as month_mrr,
                -- distribute across days in month so we can monitor daily trend
                coalesce(m.total_mrr, 0) / nullif(extract(day from (date_trunc('month', d.date_day) + interval 1 month - interval 1 day)), 0) as approx_daily_mrr
            from date_spine d
            left join monthly_mrr m
                on m.date_month = date_trunc('month', d.date_day)
        ),
        stats as (
            select
                day,
                approx_daily_mrr,
                avg(approx_daily_mrr) over (
                    order by day
                    rows between {rolling_window_days - 1} preceding and current row
                ) as rolling_mean,
                stddev_samp(approx_daily_mrr) over (
                    order by day
                    rows between {rolling_window_days - 1} preceding and current row
                ) as rolling_std
            from daily_mrr
        )
        select
            day,
            approx_daily_mrr,
            rolling_mean,
            rolling_std,
            case
                when rolling_std is null or rolling_std = 0 then 0
                else (approx_daily_mrr - rolling_mean) / rolling_std
            end as z_score
        from stats
        where rolling_mean is not null
          and abs(case when rolling_std is null or rolling_std = 0 then 0 else (approx_daily_mrr - rolling_mean) / rolling_std end) >= {z_threshold}
        order by day;
    """

    df = conn.execute(query).df()
    conn.close()

    if len(df) > 0:
        logger.warning("MRR anomaly check: found %s anomalous days (z>=%.2f)", len(df), z_threshold)
    else:
        logger.info("MRR anomaly check: no anomalies detected (z>=%.2f)", z_threshold)

    return AnomalyResult(check_name="mrr_zscore", run_at_utc=run_at, anomalies=df)


def detect_weekly_mrr_drop(
    db_path: str,
    drop_threshold_pct: float = 0.15,
    lookback_months: int = 12,
) -> AnomalyResult:
    """
    Flag month-over-month MRR drops larger than a threshold.
    (For this synthetic dataset, month-grain monitoring is more faithful than daily.)
    """
    run_at = datetime.utcnow()
    conn = _connect(db_path)

    query = f"""
        with monthly as (
            select
                date_month,
                sum(mrr_amount) as total_mrr
            from main_marts.fct_monthly_recurring_revenue
            where date_month >= date_trunc('month', current_date) - interval {lookback_months} month
            group by 1
        ),
        with_prev as (
            select
                date_month,
                total_mrr,
                lag(total_mrr) over (order by date_month) as prev_mrr
            from monthly
        )
        select
            date_month,
            total_mrr,
            prev_mrr,
            case when prev_mrr is null or prev_mrr = 0 then null else (total_mrr - prev_mrr) / prev_mrr end as pct_change
        from with_prev
        where prev_mrr is not null
          and prev_mrr > 0
          and (total_mrr - prev_mrr) / prev_mrr <= -{drop_threshold_pct}
        order by date_month;
    """

    df = conn.execute(query).df()
    conn.close()

    if len(df) > 0:
        logger.warning("MRR drop check: found %s months with drop >= %.0f%%", len(df), drop_threshold_pct * 100)
    else:
        logger.info("MRR drop check: no drops >= %.0f%%", drop_threshold_pct * 100)

    return AnomalyResult(check_name="mrr_drop_mom", run_at_utc=run_at, anomalies=df)


def detect_churn_spikes(
    db_path: str,
    rolling_months: int = 3,
    spike_multiple: float = 2.0,
    segment: Optional[str] = None,
) -> AnomalyResult:
    """
    Flag churn spikes when churn rate exceeds spike_multiple * rolling average.
    """
    run_at = datetime.utcnow()
    conn = _connect(db_path)

    segment_filter = ""
    if segment is not None:
        segment_filter = f"where customer_segment = '{segment}'"

    query = f"""
        with base as (
            select
                date_month,
                customer_segment,
                customer_churn_rate
            from main_marts.churn_analysis
            {segment_filter}
        ),
        stats as (
            select
                date_month,
                customer_segment,
                customer_churn_rate,
                avg(customer_churn_rate) over (
                    partition by customer_segment
                    order by date_month
                    rows between {rolling_months} preceding and 1 preceding
                ) as rolling_avg
            from base
        )
        select
            date_month,
            customer_segment,
            customer_churn_rate,
            rolling_avg,
            case when rolling_avg is null or rolling_avg = 0 then null else customer_churn_rate / rolling_avg end as multiple_of_avg
        from stats
        where rolling_avg is not null
          and rolling_avg > 0
          and customer_churn_rate / rolling_avg >= {spike_multiple}
        order by date_month, customer_segment;
    """

    df = conn.execute(query).df()
    conn.close()

    if len(df) > 0:
        logger.warning("Churn spike check: found %s spikes (>= %.2fx rolling avg)", len(df), spike_multiple)
    else:
        logger.info("Churn spike check: no spikes detected (>= %.2fx rolling avg)", spike_multiple)

    return AnomalyResult(check_name="churn_spike", run_at_utc=run_at, anomalies=df)


def main() -> int:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(name)s: %(message)s")

    db_path = os.environ.get("DB_PATH", "./data/warehouse/saas_analytics.duckdb")

    results = [
        detect_weekly_mrr_drop(db_path=db_path),
        detect_churn_spikes(db_path=db_path),
    ]

    any_anomalies = any(len(r.anomalies) > 0 for r in results)
    return 2 if any_anomalies else 0


if __name__ == "__main__":
    raise SystemExit(main())

